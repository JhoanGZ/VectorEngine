from __future__ import annotations

import json
from typing import Optional, Dict, Any

from .base_llm import BaseLLM


class LocalAdapter(BaseLLM):
    """
    Deterministic mock LLM provider.

    This adapter provides a stable, predictable implementation of the LLM
    contract for local development and CI environments.

    It does not perform real language generation.

    Future extension points:
        - Ollama integration
        - llama.cpp backend
        - vLLM integration
    """

    supports_response_format = False

    def __init__(self, backend: Optional[str] = None) -> None:
        self.backend = backend or "stub"

    def generate(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.2,
        model: Optional[str] = None,
        response_format: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        structured output contract used by the FinancialDecisionEngine.
        This ensures the adapter is fully substitutable with real LLM providers.
        """

        response_payload = {
            "risk_score": 0.41,
            "decision": "review",
            "key_risks": [
                "insufficient historical data",
                "mocked evaluation"
            ],
            "summary": (
                "Deterministic mock response generated by LocalAdapter. "
                "No real LLM backend is currently configured."
            )
        }

        return json.dumps(response_payload)
